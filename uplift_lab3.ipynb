{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn  as sns\n",
    "import random as rd\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import sklift.metrics as lift_metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_csv('~/all_data/uni_data/train (1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = DATA.drop('id',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600000 entries, 0 to 599999\n",
      "Data columns (total 52 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   treatment_group  600000 non-null  object \n",
      " 1   X_1              600000 non-null  float64\n",
      " 2   X_2              600000 non-null  float64\n",
      " 3   X_3              600000 non-null  float64\n",
      " 4   X_4              600000 non-null  float64\n",
      " 5   X_5              600000 non-null  float64\n",
      " 6   X_6              600000 non-null  float64\n",
      " 7   X_7              600000 non-null  float64\n",
      " 8   X_8              600000 non-null  float64\n",
      " 9   X_9              600000 non-null  float64\n",
      " 10  X_10             600000 non-null  float64\n",
      " 11  X_11             600000 non-null  float64\n",
      " 12  X_12             600000 non-null  float64\n",
      " 13  X_13             600000 non-null  float64\n",
      " 14  X_14             600000 non-null  float64\n",
      " 15  X_15             600000 non-null  float64\n",
      " 16  X_16             600000 non-null  float64\n",
      " 17  X_17             600000 non-null  float64\n",
      " 18  X_18             600000 non-null  float64\n",
      " 19  X_19             600000 non-null  float64\n",
      " 20  X_20             600000 non-null  float64\n",
      " 21  X_21             600000 non-null  float64\n",
      " 22  X_22             600000 non-null  float64\n",
      " 23  X_23             600000 non-null  float64\n",
      " 24  X_24             600000 non-null  float64\n",
      " 25  X_25             600000 non-null  float64\n",
      " 26  X_26             600000 non-null  float64\n",
      " 27  X_27             600000 non-null  float64\n",
      " 28  X_28             600000 non-null  float64\n",
      " 29  X_29             600000 non-null  float64\n",
      " 30  X_30             600000 non-null  float64\n",
      " 31  X_31             600000 non-null  float64\n",
      " 32  X_32             600000 non-null  float64\n",
      " 33  X_33             600000 non-null  float64\n",
      " 34  X_34             600000 non-null  float64\n",
      " 35  X_35             600000 non-null  float64\n",
      " 36  X_36             600000 non-null  float64\n",
      " 37  X_37             600000 non-null  float64\n",
      " 38  X_38             600000 non-null  float64\n",
      " 39  X_39             600000 non-null  float64\n",
      " 40  X_40             600000 non-null  float64\n",
      " 41  X_41             600000 non-null  float64\n",
      " 42  X_42             600000 non-null  float64\n",
      " 43  X_43             600000 non-null  float64\n",
      " 44  X_44             600000 non-null  float64\n",
      " 45  X_45             600000 non-null  float64\n",
      " 46  X_46             600000 non-null  float64\n",
      " 47  X_47             600000 non-null  float64\n",
      " 48  X_48             600000 non-null  float64\n",
      " 49  X_49             600000 non-null  float64\n",
      " 50  X_50             600000 non-null  float64\n",
      " 51  conversion       600000 non-null  int64  \n",
      "dtypes: float64(50), int64(1), object(1)\n",
      "memory usage: 238.0+ MB\n"
     ]
    }
   ],
   "source": [
    "DATA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratify by conversion\n",
    "X_train,X_test = train_test_split(DATA,train_size = 0.825,stratify = DATA['conversion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495000\n",
      "105000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['conversion'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6283, 2.4487])\n",
      "tensor([0.6283, 2.4487])\n"
     ]
    }
   ],
   "source": [
    "#compute train and val weights\n",
    "X_train_weights = compute_class_weight('balanced',y = X_train['conversion'],classes=np.array([0,1]))\n",
    "X_train_weights = torch.from_numpy(X_train_weights).to(dtype = torch.float32)\n",
    "print(X_train_weights)\n",
    "\n",
    "X_test_weights = compute_class_weight('balanced',y = X_test['conversion'],classes=np.array([0,1]))\n",
    "X_test_weights = torch.from_numpy(X_test_weights).to(dtype = torch.float32)\n",
    "print(X_test_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445500 49500\n"
     ]
    }
   ],
   "source": [
    "X_train,X_val = train_test_split(X_train,train_size = 0.9,stratify = X_train['conversion'])\n",
    "print(len(X_train),len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6283, 2.4488])\n"
     ]
    }
   ],
   "source": [
    "#just to be sure\n",
    "X_val_weights = compute_class_weight('balanced',y = X_val['conversion'],classes=np.array([0,1]))\n",
    "X_val_weights = torch.from_numpy(X_val_weights).to(dtype = torch.float32)\n",
    "print(X_val_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpliftDataset(Dataset):\n",
    "    def __init__(self,dataset_itself):\n",
    "        self.data = dataset_itself\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,indx):\n",
    "        df_row = self.data.iloc[indx,:]\n",
    "        treatment = (df_row.iloc[0] == 'treatment')\n",
    "        df_row.iloc[0] = 1 if treatment else 0\n",
    "        X,y = df_row.iloc[:-1],df_row.iloc[-1]\n",
    "        X,y = torch.from_numpy(X.to_numpy(dtype = np.float32)),torch.tensor(int(y))\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_dataset = UpliftDataset(X_train)\n",
    "# X_test_dataset = UpliftDataset(X_test)\n",
    "# X_val_dataset = UpliftDataset(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(X_train_dataset,'/home/luchian/prog/uni_prog/uni_data/X_train_uplift.pth')\n",
    "# torch.save(X_test_dataset,'/home/luchian/prog/uni_prog/uni_data/X_test_uplift.pth')\n",
    "# torch.save(X_val_dataset,'/home/luchian/prog/uni_prog/uni_data/X_val_uplift.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4757/855856896.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  X_train_dataset = torch.load('/home/luchian/prog/uni_prog/uni_data/X_train_uplift.pth')\n",
      "/tmp/ipykernel_4757/855856896.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  X_test_dataset = torch.load('/home/luchian/prog/uni_prog/uni_data/X_test_uplift.pth')\n",
      "/tmp/ipykernel_4757/855856896.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  X_val_dataset = torch.load('/home/luchian/prog/uni_prog/uni_data/X_val_uplift.pth')\n"
     ]
    }
   ],
   "source": [
    "X_train_dataset = torch.load('/home/luchian/prog/uni_prog/uni_data/X_train_uplift.pth')\n",
    "X_test_dataset = torch.load('/home/luchian/prog/uni_prog/uni_data/X_test_uplift.pth')\n",
    "X_val_dataset = torch.load('/home/luchian/prog/uni_prog/uni_data/X_val_uplift.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_dataset) + len(X_test_dataset) + len(X_val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the S model\n",
    "class SModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rel = nn.LeakyReLU(0.05)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.drop = nn.Dropout1d(p = 0.15)\n",
    "\n",
    "        self.norm1 = nn.BatchNorm1d(128)\n",
    "        self.norm2 = nn.BatchNorm1d(128)\n",
    "        self.norm3 = nn.BatchNorm1d(128)\n",
    "        self.norm4 = nn.BatchNorm1d(1)\n",
    "\n",
    "        self.lin1 = nn.Linear(51,128)\n",
    "        self.lin2 = nn.Linear(128,128)\n",
    "        self.lin3 = nn.Linear(128,128)\n",
    "        self.lin4 = nn.Linear(128,1)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        y = self.rel(self.lin1(X))\n",
    "        y = self.norm1(y)\n",
    "        y = self.drop(y)\n",
    "\n",
    "        y = self.rel(self.lin2(y))\n",
    "        y = self.norm2(y)\n",
    "        y = self.drop(y)\n",
    "\n",
    "        y = self.rel(self.lin3(y))\n",
    "        y = self.norm3(y)\n",
    "        y = self.drop(y)\n",
    "     \n",
    "        y = self.lin4(y)\n",
    "        y = self.norm4(y)\n",
    "        y = self.sigm(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model,some_dataset,dev = 'cuda'):\n",
    "    model.eval()\n",
    "    N = len(some_dataset)\n",
    "    main_list = []\n",
    "    main_loader = DataLoader(dataset = some_dataset,shuffle = True,batch_size=1)\n",
    "    for X,y in main_loader:\n",
    "        X,y = X.to(device = dev),y.to(device = dev)\n",
    "        y_pred = model(X)\n",
    "        label = 1 if y_pred > 0.5 else 0\n",
    "        if label == y[0]:\n",
    "            main_list.append(1)\n",
    "    return round(sum(main_list)/N,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def dataset_loss(model,dev,dataset,criterion,target_weights,bat_size):\n",
    "    \"\"\"calculates the loss on the dataset given the model and loss\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    val_loss_over_batches = []\n",
    "\n",
    "    main_loader = DataLoader(dataset = dataset,shuffle = True,batch_size = bat_size)\n",
    "    for X,y in main_loader:\n",
    "        X,y = X.to(device = dev),y.reshape(y.shape[0],1).to(device = dev,dtype = torch.float32)\n",
    "        #some computation for weights\n",
    "        yy = torch.clone(y)\n",
    "        yy[yy == 0] = target_weights[0]\n",
    "        yy[yy == 1] = target_weights[1]\n",
    "        main_weights = yy.reshape(-1)\n",
    "        #creating the loss function\n",
    "        loss = criterion(reduction = 'sum',weight = main_weights)\n",
    "        #calculating the loss\n",
    "        y_pred = model(X)\n",
    "        the_loss = loss(y_pred.T,y.T)\n",
    "        val_loss_over_batches.append(round(the_loss.item(),5))\n",
    "    return sum(val_loss_over_batches)/len(main_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,dev,Train_dataset,Val_dataset,optim,optim_params,criterion,target_weights,bat_size,epoch,pre_optim = False\n",
    "):\n",
    "    \"\"\"Train and model given the parameters\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    train_loader = DataLoader(dataset = Train_dataset,shuffle=True,batch_size = bat_size)\n",
    "\n",
    "    optimizer = optim(model.parameters(),**optim_params)\n",
    "    #if the optimizer is pre-dertermined\n",
    "    if pre_optim:\n",
    "        optim(model.parameters(),**optim_params).load_state_dict(pre_optim.state_dict())\n",
    "\n",
    "    train_losses_over_batches = []\n",
    "    val_accs = [-float('inf')]\n",
    "\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        for X,y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            X,y = X.to(device = dev),y.reshape(y.shape[0],1).to(device = dev,dtype = torch.float32)\n",
    "            #some computation for weights\n",
    "            yy = torch.clone(y)\n",
    "            yy[yy == 0] = target_weights[0]\n",
    "            yy[yy == 1] = target_weights[1]\n",
    "            main_weights = yy.reshape(-1)\n",
    "            #for each such weight we create separte loss function\n",
    "            loss = criterion(reduction = 'sum',weight = main_weights) \n",
    "            #prediction and backprop\n",
    "            y_pred = model(X)\n",
    "            the_loss = loss(y_pred.T,y.T)\n",
    "            the_loss.backward()\n",
    "            optimizer.step()\n",
    "            #saving the losses \n",
    "            train_losses_over_batches.append(round(the_loss.item(),5))\n",
    "        #every 3-rd epoch pring results \n",
    "        #stop iteration if the error on val score has increased\n",
    "        current_val_acc = get_accuracy(model,Val_dataset,dev = 'cuda')\n",
    "        val_accs.append(current_val_acc)\n",
    "        print(f'Epoch #{ep+1} | Train Loss: {train_losses_over_batches[-1]} | Val accuracy: {val_accs[-1]}')\n",
    "        if val_accs[-1] < val_accs[-2]:\n",
    "            print('\\n\\nThe accuracy on Val dataset has decreased. Stopping training iterations....')\n",
    "            break\n",
    "    return train_losses_over_batches,val_accs,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "smodel = SModel().to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 | Train Loss: 43.67865 | Val accuracy: 0.83285\n",
      "Epoch #2 | Train Loss: 40.45711 | Val accuracy: 0.83293\n",
      "Epoch #3 | Train Loss: 42.93871 | Val accuracy: 0.8381\n",
      "Epoch #4 | Train Loss: 40.36104 | Val accuracy: 0.82469\n",
      "\n",
      "\n",
      "The accuracy on Val dataset has decreased. Stopping training iterations....\n"
     ]
    }
   ],
   "source": [
    "res1 = train_model(model = smodel,\n",
    "                   dev = 'cuda',\n",
    "                   Train_dataset = X_train_dataset,Val_dataset = X_val_dataset,\n",
    "                   optim = torch.optim.Adam,\n",
    "                   optim_params={'betas': (0.9,0.98),'lr': 0.08},\n",
    "                   criterion=nn.BCELoss,\n",
    "                   target_weights = X_train_weights,\n",
    "                   bat_size = 64,\n",
    "                   epoch = 3000\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 | Train Loss: 26.67966 | Val accuracy: 0.83545\n",
      "Epoch #2 | Train Loss: 39.51958 | Val accuracy: 0.83273\n",
      "\n",
      "\n",
      "The accuracy on Val dataset has decreased. Stopping training iterations....\n"
     ]
    }
   ],
   "source": [
    "res2 = train_model(model = smodel,\n",
    "                   dev = 'cuda',\n",
    "                   Train_dataset = X_train_dataset,Val_dataset = X_val_dataset,\n",
    "                   optim = torch.optim.Adam,\n",
    "                   optim_params={'betas': (0.9,0.98),'lr': 0.005},\n",
    "                   criterion=nn.BCELoss,\n",
    "                   target_weights = X_train_weights,\n",
    "                   bat_size = 64,\n",
    "                   epoch = 3000,\n",
    "                   pre_optim=False\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL1\n",
    "# torch.save(smodel.state_dict(),'/home/luchian/prog/uni_prog/uni_data/MyModels/S_model_1.pth')\n",
    "# smodel.load_state_dict(torch.load('/home/luchian/prog/uni_prog/uni_data/MyModels/S_model_1.pth',weights_only = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MODEL2\n",
    "# torch.save(smodel.state_dict(),'/home/luchian/prog/uni_prog/uni_data/MyModels/S_model_2.pth')\n",
    "# smodel.load_state_dict(torch.load('/home/luchian/prog/uni_prog/uni_data/MyModels/S_model_2.pth',weights_only = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL3\n",
    "# torch.save(smodel.state_dict(),'/home/luchian/prog/uni_prog/uni_data/MyModels/S_model_3.pth')\n",
    "# smodel.load_state_dict(torch.load('/home/luchian/prog/uni_prog/uni_data/MyModels/S_model_3.pth',weights_only = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uplift_metrics(y_true,uplift,treatment):\n",
    "    upliftk_group = lift_metrics.uplift_at_k(y_true = y_true,uplift = uplift,treatment = treatment,strategy = 'by_group',k = 0.35)\n",
    "    upliftk_overall = lift_metrics.uplift_at_k(y_true = y_true,uplift = uplift,treatment = treatment,strategy = 'overall',k = 0.35)\n",
    "\n",
    "    qini_auc = lift_metrics.qini_auc_score(y_true = y_true,uplift = uplift,treatment = treatment)\n",
    "\n",
    "    uplift_auc = lift_metrics.uplift_auc_score(y_true = y_true,uplift = uplift,treatment = treatment)\n",
    "\n",
    "    #weighted average\n",
    "    wau = lift_metrics.weighted_average_uplift(y_true = y_true,uplift = uplift,treatment = treatment,strategy = 'by_group')\n",
    "    wau_all = lift_metrics.weighted_average_uplift(y_true = y_true,uplift = uplift,treatment = treatment,strategy = 'by_group')\n",
    "\n",
    "    print(f'Uplift at top 30% by group: {upliftk_group:.3f} | Uplift at top 30% by overall: {upliftk_overall:.3f}')\n",
    "    print(f'Weighted average uplift by group: {wau:.3f} | Weighted average uplift by overall: {wau_all:.3f}  ')\n",
    "    print(f'AUUC by group: {uplift_auc:.3f}')\n",
    "    print(f'AUQC by group: {qini_auc:.3f}')\n",
    "\n",
    "    metric_dict = {'uplift@k_group':upliftk_group,'uplift@k_overall':upliftk_overall,\n",
    "                   'qini_auc':qini_auc,'uplift_auc':uplift_auc,'WAU':wau,'WAU_all':wau_all}\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestUpliftDataset(Dataset):\n",
    "    def __init__(self,dataset_itself):\n",
    "        self.data = dataset_itself\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,indx):\n",
    "        df_row = self.data.iloc[indx,:]\n",
    "        treatment = (df_row.iloc[0] == 'treatment')\n",
    "        df_row.iloc[0] = 1 if treatment else 0\n",
    "        X,y = df_row.iloc[:-1],df_row.iloc[-1]\n",
    "        X,y = torch.from_numpy(X.to_numpy(dtype = np.float32)),torch.tensor(int(y))\n",
    "        return X[1:],X[0],y.to(dtype = torch.float32) #X,treatment,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TestUpliftDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_up_t_y(model,test_dataset):\n",
    "    model.eval()\n",
    "    main_dataset = test_dataset\n",
    "    main_dict = {'uplift':[],'treatment':[],'y_true':[]}\n",
    "    for X,t,y in main_dataset:\n",
    "        X = X.to(device = 'cuda')\n",
    "        w1 = torch.tensor([1],dtype = torch.float32).to(device = 'cuda')\n",
    "        w0 = torch.tensor([0],dtype = torch.float32).to(device = 'cuda')\n",
    "        X1 = torch.cat([w1,X],axis = 0).reshape(1,X.shape[0]+1)\n",
    "        X2 = torch.cat([w0,X],axis = 0).reshape(1,X.shape[0]+1)\n",
    "        y_pred_1 = model(X1).cpu()\n",
    "        y_pred_0 = model(X2).cpu()\n",
    "        #getting_values\n",
    "        uplift = (y_pred_1-y_pred_0).item()\n",
    "        t = t.item()\n",
    "        y = y.item()\n",
    "        #inserting in dictionary\n",
    "        main_dict['uplift'].append(uplift)\n",
    "        main_dict['treatment'].append(t)\n",
    "        main_dict['y_true'].append(y)\n",
    "    return main_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dict = get_up_t_y(smodel,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uplift at top 30% by group: 0.190 | Uplift at top 30% by overall: 0.190\n",
      "Weighted average uplift by group: 0.046 | Weighted average uplift by overall: 0.046  \n",
      "AUUC by group: 0.149\n",
      "AUQC by group: 0.215\n"
     ]
    }
   ],
   "source": [
    "get_uplift_metrics(y_true = np.array(s_dict['y_true']),\n",
    "                         uplift = np.array(s_dict['uplift']),\n",
    "                         treatment = np.array(s_dict['treatment']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-MyPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
